{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# q1_softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def softmax(x):\n",
    "    \n",
    "    if len(x.shape) > 1:\n",
    "        tmp = np.max(x, axis = 1)\n",
    "        x -= tmp.reshape((x.shape[0], 1))\n",
    "        x = np.exp(x)\n",
    "        tmp = np.sum(x, axis = 1)\n",
    "        x /= tmp.reshape((x.shape[0], 1))\n",
    "    else:\n",
    "        tmp = np.max(x)\n",
    "        x -= tmp\n",
    "        x = np.exp(x)\n",
    "        tmp = np.sum(x)\n",
    "        x /= tmp\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic tests...\n",
      "[ 0.26894142  0.73105858]\n",
      "[[ 0.26894142  0.73105858]\n",
      " [ 0.26894142  0.73105858]]\n",
      "[[ 0.73105858  0.26894142]]\n",
      "Passed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_softmax_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests to get you started. \n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "    print \"Running basic tests...\"\n",
    "    test1 = softmax(np.array([1,2]))\n",
    "    print test1\n",
    "    assert np.amax(np.fabs(test1 - np.array(\n",
    "        [0.26894142,  0.73105858]))) <= 1e-6\n",
    "\n",
    "    test2 = softmax(np.array([[1001,1002],[3,4]]))\n",
    "    print test2\n",
    "    assert np.amax(np.fabs(test2 - np.array(\n",
    "        [[0.26894142, 0.73105858], [0.26894142, 0.73105858]]))) <= 1e-6\n",
    "\n",
    "    test3 = softmax(np.array([[-1001,-1002]]))\n",
    "    print test3\n",
    "    assert np.amax(np.fabs(test3 - np.array(\n",
    "        [0.73105858, 0.26894142]))) <= 1e-6\n",
    "\n",
    "    print \"Passed!\\n\"\n",
    "    \n",
    "test_softmax_basic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# q2 sigmoid\n",
    "- sigmoid\n",
    "- gradient of sigmoid\n",
    "- test for the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic tests...\n",
      "[[ 0.73105858  0.88079708]\n",
      " [ 0.26894142  0.11920292]]\n",
      "[[ 0.19661193  0.10499359]\n",
      " [ 0.19661193  0.10499359]]\n",
      "You should verify these results!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function for the input here.\n",
    "    \"\"\"\n",
    "    x = 1. / (1 + np.exp(-x))\n",
    "    \n",
    "    return x\n",
    "\n",
    "def sigmoid_grad(f):\n",
    "    \"\"\"\n",
    "    Compute the gradient for the sigmoid function here. Note that\n",
    "    for this implementation, the input f should be the sigmoid\n",
    "    function value of your original input x. \n",
    "    \"\"\"\n",
    "    f = f * (1-f)\n",
    "    \n",
    "    return f\n",
    "\n",
    "def test_sigmoid_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests to get you started. \n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "    print \"Running basic tests...\"\n",
    "    x = np.array([[1, 2], [-1, -2]])\n",
    "    f = sigmoid(x)\n",
    "    g = sigmoid_grad(f)\n",
    "    print f\n",
    "    assert np.amax(f - np.array([[0.73105858, 0.88079708], \n",
    "        [0.26894142, 0.11920292]])) <= 1e-6\n",
    "    print g\n",
    "    assert np.amax(g - np.array([[0.19661193, 0.10499359],\n",
    "        [0.19661193, 0.10499359]])) <= 1e-6\n",
    "    print \"You should verify these results!\\n\"\n",
    "\n",
    "test_sigmoid_basic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# q2 Gradient checking\n",
    "Reference : http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sanity checks...\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def gradcheck_naive(f, x):\n",
    "    \"\"\" \n",
    "    Gradient check for a function f \n",
    "    - f should be a function that takes a single argument and outputs the cost and its gradients\n",
    "    - x is the point (numpy array) to check the gradient at\n",
    "    - Reference : http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/ \n",
    "    \"\"\" \n",
    "\n",
    "    rndstate = random.getstate()\n",
    "    random.setstate(rndstate)  \n",
    "    fx, gd = f(x) # Evaluate function value at original point\n",
    "    h = 1e-4\n",
    "\n",
    "    # Iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        \n",
    "        x[ix] += h\n",
    "        random.setstate(rndstate)\n",
    "        fx_p, _ = f(x)\n",
    "        x[ix] -= 2*h\n",
    "        random.setstate(rndstate)\n",
    "        fx_n, _ = f(x)\n",
    "        x[ix] += h\n",
    "        grad = (fx_p - fx_n) / (2*h)\n",
    "        \n",
    "        diff = abs(grad - gd[ix]) / max(1, abs(grad), abs(gd[ix]))\n",
    "        \n",
    "        if diff > 1e-5:\n",
    "            print \"Gradient check failed.\"\n",
    "            print \"First gradient error found at index %s\" % str(ix)\n",
    "            print \"Your gradient: %f \\t Numerical gradient: %f\" % (gd[ix], grad)\n",
    "            return\n",
    "\n",
    "    \n",
    "        it.iternext() # Step to next dimension\n",
    "\n",
    "    print \"Gradient check passed!\"\n",
    "\n",
    "def sanity_check():\n",
    "    \"\"\"\n",
    "    Some basic sanity checks.\n",
    "    \"\"\"\n",
    "    quad = lambda x: (np.sum(x ** 2), x * 2)\n",
    "\n",
    "    print \"Running sanity checks...\"\n",
    "    gradcheck_naive(quad, np.array(123.456))      # scalar test\n",
    "    gradcheck_naive(quad, np.random.randn(3,))    # 1-D test\n",
    "    gradcheck_naive(quad, np.random.randn(4,5))   # 2-D test\n",
    "    print \"\"\n",
    "\n",
    "sanity_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# q2 neural net\n",
    "Reference : http://neuralnetworksanddeeplearning.com/chap2.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sanity check...\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "def forward_backward_prop(data, labels, params, dimensions):\n",
    "    \"\"\" \n",
    "    Forward and backward propagation for a two-layer sigmoidal network \n",
    "    \n",
    "    Compute the forward propagation and for the cross entropy cost,\n",
    "    and backward propagation for the gradients for all parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    ### Unpack network parameters (do not modify)\n",
    "    ofs = 0\n",
    "    Dx, H, Dy = (dimensions[0], dimensions[1], dimensions[2])\n",
    "\n",
    "    W1 = np.reshape(params[ofs:ofs+ Dx * H], (Dx, H))\n",
    "    ofs += Dx * H\n",
    "    b1 = np.reshape(params[ofs:ofs + H], (1, H))\n",
    "    ofs += H\n",
    "    W2 = np.reshape(params[ofs:ofs + H * Dy], (H, Dy))\n",
    "    ofs += H * Dy\n",
    "    b2 = np.reshape(params[ofs:ofs + Dy], (1, Dy))\n",
    "\n",
    "    ### YOUR CODE HERE: forward propagation\n",
    "    hidden = sigmoid( np.dot(data, W1) + b1)\n",
    "    prediction = softmax( np.dot(hidden, W2) + b2)\n",
    "    cost = -np.sum(np.log(prediction) * labels)\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    ### YOUR CODE HERE: backward propagation\n",
    "    ### reference : http://neuralnetworksanddeeplearning.com/chap2.html \n",
    "    delta = prediction - labels\n",
    "    gradW2 = np.dot(hidden.T, delta) \n",
    "    gradb2 = np.sum(delta, axis = 0)\n",
    "    delta = delta.dot(W2.T) * sigmoid_grad(hidden)\n",
    "    gradW1 = data.T.dot(delta)\n",
    "    gradb1 = np.sum(delta, axis = 0)\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    ### Stack gradients (do not modify)\n",
    "    grad = np.concatenate((gradW1.flatten(), gradb1.flatten(), \n",
    "        gradW2.flatten(), gradb2.flatten()))\n",
    "    \n",
    "    return cost, grad\n",
    "\n",
    "def neural_sanity_check():\n",
    "    \"\"\"\n",
    "    Set up fake data and parameters for the neural network, and test using \n",
    "    gradcheck.\n",
    "    \"\"\"\n",
    "    print \"Running sanity check...\"\n",
    "\n",
    "    N = 20\n",
    "    dimensions = [10, 5, 10]\n",
    "    data = np.random.randn(N, dimensions[0])   # each row will be a datum\n",
    "    labels = np.zeros((N, dimensions[2]))\n",
    "    for i in xrange(N):\n",
    "        labels[i,random.randint(0,dimensions[2]-1)] = 1\n",
    "    \n",
    "    params = np.random.randn((dimensions[0] + 1) * dimensions[1] + (\n",
    "        dimensions[1] + 1) * dimensions[2], )\n",
    "\n",
    "    gradcheck_naive(lambda params: forward_backward_prop(data, labels, params,\n",
    "        dimensions), params)\n",
    "\n",
    "neural_sanity_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# q3 word2vec \n",
    "## normalize rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing normalizeRows...\n",
      "[[ 0.6         0.8       ]\n",
      " [ 0.4472136   0.89442719]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def normalizeRows(x):\n",
    "    \"\"\" Row normalization function \"\"\"\n",
    "    # Implement a function that normalizes each row of a matrix to have unit length\n",
    "    \n",
    "    length = np.sqrt( np.sum(x**2, axis = 1))\n",
    "    x /= length.reshape(x.shape[0],1)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def test_normalize_rows():\n",
    "    print \"Testing normalizeRows...\"\n",
    "    x = normalizeRows(np.array([[3.0,4.0],[1, 2]])) \n",
    "    # the result should be [[0.6, 0.8], [0.4472, 0.8944]]\n",
    "    print x\n",
    "    assert (x.all() == np.array([[0.6, 0.8], [0.4472, 0.8944]]).all())\n",
    "    print \"\"\n",
    "    \n",
    "test_normalize_rows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax cost and gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmaxCostAndGradient(predicted, target, outputVectors, dataset):\n",
    "    \"\"\" Softmax cost function for word2vec models \"\"\"\n",
    "    \n",
    "    # Implement the cost and gradients for one predicted word vector  \n",
    "    # and one target word vector as a building block for word2vec     \n",
    "    # models, assuming the softmax prediction function and cross      \n",
    "    # entropy loss.                                                   \n",
    "    \n",
    "    # Inputs:                                                         \n",
    "    # - predicted: numpy ndarray, predicted word vector (\\hat{v} in \n",
    "    #   the written component or \\hat{r} in an earlier version)\n",
    "    # - target: integer, the index of the target word               \n",
    "    # - outputVectors: \"output\" vectors (as rows) for all tokens     \n",
    "    # - dataset: needed for negative sampling, unused here.         \n",
    "    \n",
    "    # Outputs:                                                        \n",
    "    # - cost: cross entropy cost for the softmax word prediction    \n",
    "    # - gradPred: the gradient with respect to the predicted word   \n",
    "    #        vector                                                \n",
    "    # - grad: the gradient with respect to all the other word        \n",
    "    #        vectors                                               \n",
    "    \n",
    "    # We will not provide starter code for this function, but feel    \n",
    "    # free to reference the code you previously wrote for this        \n",
    "    # assignment!                                                  \n",
    "    \n",
    "    ### Check solution for assignment q3 for reference\n",
    "    ### predicted =  1,D ... output  =  V,D\n",
    "    ### cost is cross entropy loss of probabilities[target]\n",
    "    probabilities = softmax( np.dot(predicted, outputVectors.T))\n",
    "    cost = - np.log(probabilities[target])\n",
    "    \n",
    "    ### delta = y_predicted - y, y_i = 0 except i == target\n",
    "    delta = probabilities\n",
    "    delta[target] -= 1\n",
    "    \n",
    "    gradPred = np.dot(delta.T,outputVectors)\n",
    "    grad = np.dot(delta.reshape(delta.shape[0],1),predicted.reshape(1,predicted.shape[0]))\n",
    "    \n",
    "    return cost, gradPred, grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# negative sample cost and gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def negSamplingCostAndGradient(predicted, target, outputVectors, dataset, \n",
    "    K=10):\n",
    "    \"\"\" Negative sampling cost function for word2vec models \"\"\"\n",
    "\n",
    "    # Implement the cost and gradients for one predicted word vector  \n",
    "    # and one target word vector as a building block for word2vec     \n",
    "    # models, using the negative sampling technique. K is the sample  \n",
    "    # size. You might want to use dataset.sampleTokenIdx() to sample  \n",
    "    # a random word index. \n",
    "    # \n",
    "    # Note: See test_word2vec below for dataset's initialization.\n",
    "    #                                       \n",
    "    # Input/Output Specifications: same as softmaxCostAndGradient     \n",
    "    # We will not provide starter code for this function, but feel    \n",
    "    # free to reference the code you previously wrote for this        \n",
    "    # assignment!\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    out_indices = [target]\n",
    "\n",
    "    for k in range(K):\n",
    "        index = dataset.sampleTokenIdx()\n",
    "        while index == target:\n",
    "            index = dataset.sampleTokenIdx()\n",
    "        out_indices += [index]\n",
    "\n",
    "    out_vectors = outputVectors[out_indices,:]\n",
    "\n",
    "    labels = np.array([1] + [-1]* K)\n",
    "\n",
    "    prediction = sigmoid( out_vectors.dot(predicted) * labels )\n",
    "    cost = - np.sum(np.log(prediction))\n",
    "\n",
    "    delta = labels * (prediction - 1)\n",
    "\n",
    "\n",
    "    gradPred = delta.dot( out_vectors)\n",
    "\n",
    "    gradTemp = (delta.reshape(K+1, 1)).dot(predicted.reshape(1, predicted.shape[0]))\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    grad = np.zeros(outputVectors.shape)\n",
    "    for index in range(K+1):\n",
    "        grad[out_indices[index]] += gradTemp[index]\n",
    "\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradPred, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skipgram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def skipgram(currentWord, C, contextWords, tokens, inputVectors, outputVectors, \n",
    "    dataset, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "    \"\"\" Skip-gram model in word2vec \"\"\"\n",
    "\n",
    "    # Implement the skip-gram model in this function.\n",
    "\n",
    "    # Inputs:                                                         \n",
    "    # - currrentWord: a string of the current center word           \n",
    "    # - C: integer, context size                                    \n",
    "    # - contextWords: list of no more than 2*C strings, the context words                                               \n",
    "    # - tokens: a dictionary that maps words to their indices in    \n",
    "    #      the word vector list                                \n",
    "    # - inputVectors: \"input\" word vectors (as rows) for all tokens           \n",
    "    # - outputVectors: \"output\" word vectors (as rows) for all tokens         \n",
    "    # - word2vecCostAndGradient: the cost and gradient function for \n",
    "    #      a prediction vector given the target word vectors,  \n",
    "    #      could be one of the two cost functions you          \n",
    "    #      implemented above\n",
    "\n",
    "    # Outputs:                                                        \n",
    "    # - cost: the cost function value for the skip-gram model       \n",
    "    # - grad: the gradient with respect to the word vectors         \n",
    "    # We will not provide starter code for this function, but feel    \n",
    "    # free to reference the code you previously wrote for this        \n",
    "    # assignment!\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    # v_c\n",
    "    index_center = tokens[currentWord]\n",
    "    predicted = inputVectors[index_center, :]  \n",
    "    \n",
    "    cost = 0\n",
    "    gradIn = np.zeros(inputVectors.shape)\n",
    "    gradOut = np.zeros(outputVectors.shape)\n",
    "    \n",
    "    \n",
    "    for word in contextWords:\n",
    "        index = tokens[word]\n",
    "        cost_w, gradP, grad = word2vecCostAndGradient(predicted, index, outputVectors, dataset)\n",
    "        cost += cost_w\n",
    "        gradIn[index_center, : ] += gradP\n",
    "        gradOut += grad\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradIn, gradOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cbow(currentWord, C, contextWords, tokens, inputVectors, outputVectors, \n",
    "    dataset, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "    \"\"\" CBOW model in word2vec \"\"\"\n",
    "\n",
    "    # Implement the continuous bag-of-words model in this function.            \n",
    "    # Input/Output specifications: same as the skip-gram model        \n",
    "    # We will not provide starter code for this function, but feel    \n",
    "    # free to reference the code you previously wrote for this        \n",
    "    # assignment!\n",
    "\n",
    "    #################################################################\n",
    "    # IMPLEMENTING CBOW IS EXTRA CREDIT, DERIVATIONS IN THE WRIITEN #\n",
    "    # ASSIGNMENT ARE NOT!                                           #  \n",
    "    #################################################################\n",
    "    \n",
    "    cost = 0\n",
    "    gradIn = np.zeros(inputVectors.shape)\n",
    "    gradOut = np.zeros(outputVectors.shape)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    indexes = [tokens[word] for word in contextWords]\n",
    "    predicted = np.zeros(( inputVectors.shape[1]))\n",
    "    \n",
    "    for index in indexes:\n",
    "        predicted += inputVectors[index,:]\n",
    "    \n",
    "    #print \"indexes: \" ,indexes\n",
    "    \n",
    "    cost, gradP, gradOut = word2vecCostAndGradient(predicted, tokens[currentWord], outputVectors, dataset)\n",
    "    \n",
    "    for index in indexes:\n",
    "        gradIn[index,:] += gradP \n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradIn, gradOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Gradient check for skip-gram ====\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "\n",
      "==== Gradient check for CBOW      ====\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "\n",
      "=== Results ===\n",
      "(11.166109001533981, array([[ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [-1.26947339, -1.36873189,  2.45158957],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[-0.41045956,  0.18834851,  1.43272264],\n",
      "       [ 0.38202831, -0.17530219, -1.33348241],\n",
      "       [ 0.07009355, -0.03216399, -0.24466386],\n",
      "       [ 0.09472154, -0.04346509, -0.33062865],\n",
      "       [-0.13638384,  0.06258276,  0.47605228]]))\n",
      "(13.959405258751875, array([[ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [-4.12113804, -1.67347865, -1.5049951 ],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[-0.49853822,  0.22876535,  1.74016407],\n",
      "       [-0.02700439,  0.01239157,  0.09425972],\n",
      "       [-0.68292656,  0.31337605,  2.38377767],\n",
      "       [-0.84273629,  0.3867083 ,  2.94159878],\n",
      "       [-0.16124059,  0.07398883,  0.5628156 ]]))\n",
      "(0.79899580109066481, array([[ 0.23330542, -0.51643128, -0.8281311 ],\n",
      "       [ 0.11665271, -0.25821564, -0.41406555],\n",
      "       [ 0.11665271, -0.25821564, -0.41406555],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[ 0.80954933,  0.21962514, -0.54095764],\n",
      "       [-0.03556575, -0.00964874,  0.02376577],\n",
      "       [-0.13016109, -0.0353118 ,  0.08697634],\n",
      "       [-0.1650812 , -0.04478539,  0.11031068],\n",
      "       [-0.47874129, -0.1298792 ,  0.31990485]]))\n",
      "(7.7630889928618743, array([[-3.24112111, -1.89068433, -2.69507064],\n",
      "       [-1.62056055, -0.94534217, -1.34753532],\n",
      "       [-1.62056055, -0.94534217, -1.34753532],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[ 0.21992784,  0.0596649 , -0.14696034],\n",
      "       [-1.37825047, -0.37390982,  0.92097553],\n",
      "       [-1.55404334, -0.42160121,  1.03844397],\n",
      "       [-1.72636934, -0.46835207,  1.15359577],\n",
      "       [-2.36749007, -0.64228369,  1.58200593]]))\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# Testing functions below. DO NOT MODIFY!   #\n",
    "#############################################\n",
    "\n",
    "def word2vec_sgd_wrapper(word2vecModel, tokens, wordVectors, dataset, C, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "    batchsize = 50\n",
    "    cost = 0.0\n",
    "    grad = np.zeros(wordVectors.shape)\n",
    "    N = wordVectors.shape[0]\n",
    "    inputVectors = wordVectors[:N/2,:]\n",
    "    outputVectors = wordVectors[N/2:,:]\n",
    "    for i in xrange(batchsize):\n",
    "        C1 = random.randint(1,C)\n",
    "        centerword, context = dataset.getRandomContext(C1)\n",
    "        \n",
    "        if word2vecModel == skipgram:\n",
    "            denom = 1\n",
    "        else:\n",
    "            denom = 1\n",
    "        \n",
    "        c, gin, gout = word2vecModel(centerword, C1, context, tokens, inputVectors, outputVectors, dataset, word2vecCostAndGradient)\n",
    "        cost += c / batchsize / denom\n",
    "        grad[:N/2, :] += gin / batchsize / denom\n",
    "        grad[N/2:, :] += gout / batchsize / denom\n",
    "        \n",
    "    return cost, grad\n",
    "\n",
    "def test_word2vec():\n",
    "    # Interface to the dataset for negative sampling\n",
    "    dataset = type('dummy', (), {})()\n",
    "    def dummySampleTokenIdx():\n",
    "        return random.randint(0, 4)\n",
    "\n",
    "    def getRandomContext(C):\n",
    "        tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "        return tokens[random.randint(0,4)], [tokens[random.randint(0,4)] \\\n",
    "           for i in xrange(2*C)]\n",
    "    dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "    dataset.getRandomContext = getRandomContext\n",
    "\n",
    "    random.seed(31415)\n",
    "    np.random.seed(9265)\n",
    "    dummy_vectors = normalizeRows(np.random.randn(10,3))\n",
    "    dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])\n",
    "    print \"==== Gradient check for skip-gram ====\"\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(skipgram, dummy_tokens, vec, dataset, 5), dummy_vectors)\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(skipgram, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient), dummy_vectors)\n",
    "    print \"\\n==== Gradient check for CBOW      ====\"\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(cbow, dummy_tokens, vec, dataset, 5), dummy_vectors)\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(cbow, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient), dummy_vectors)\n",
    "\n",
    "    print \"\\n=== Results ===\"\n",
    "    print skipgram(\"c\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset)\n",
    "    print skipgram(\"c\", 1, [\"a\", \"b\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset, negSamplingCostAndGradient)\n",
    "    print cbow(\"a\", 2, [\"a\", \"b\", \"c\", \"a\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset)\n",
    "    print cbow(\"a\", 2, [\"a\", \"b\", \"a\", \"c\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset, negSamplingCostAndGradient)\n",
    "\n",
    "test_word2vec()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
