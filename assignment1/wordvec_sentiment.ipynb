{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# q1_softmax "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def softmax(x):\n",
    "    \n",
    "    if len(x.shape) > 1:\n",
    "        tmp = np.max(x, axis = 1)\n",
    "        x -= tmp.reshape((x.shape[0], 1))\n",
    "        x = np.exp(x)\n",
    "        tmp = np.sum(x, axis = 1)\n",
    "        x /= tmp.reshape((x.shape[0], 1))\n",
    "    else:\n",
    "        tmp = np.max(x)\n",
    "        x -= tmp\n",
    "        x = np.exp(x)\n",
    "        tmp = np.sum(x)\n",
    "        x /= tmp\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test for softmax function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic tests...\n",
      "[ 0.26894142  0.73105858]\n",
      "[[ 0.26894142  0.73105858]\n",
      " [ 0.26894142  0.73105858]]\n",
      "[[ 0.73105858  0.26894142]]\n",
      "Passed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_softmax_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests to get you started. \n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "    print \"Running basic tests...\"\n",
    "    test1 = softmax(np.array([1,2]))\n",
    "    print test1\n",
    "    assert np.amax(np.fabs(test1 - np.array(\n",
    "        [0.26894142,  0.73105858]))) <= 1e-6\n",
    "\n",
    "    test2 = softmax(np.array([[1001,1002],[3,4]]))\n",
    "    print test2\n",
    "    assert np.amax(np.fabs(test2 - np.array(\n",
    "        [[0.26894142, 0.73105858], [0.26894142, 0.73105858]]))) <= 1e-6\n",
    "\n",
    "    test3 = softmax(np.array([[-1001,-1002]]))\n",
    "    print test3\n",
    "    assert np.amax(np.fabs(test3 - np.array(\n",
    "        [0.73105858, 0.26894142]))) <= 1e-6\n",
    "\n",
    "    print \"Passed!\\n\"\n",
    "    \n",
    "test_softmax_basic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# q2 sigmoid\n",
    "- sigmoid\n",
    "- gradient of sigmoid\n",
    "- test for the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running basic tests...\n",
      "[[ 0.73105858  0.88079708]\n",
      " [ 0.26894142  0.11920292]]\n",
      "[[ 0.19661193  0.10499359]\n",
      " [ 0.19661193  0.10499359]]\n",
      "You should verify these results!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid function for the input here.\n",
    "    \"\"\"\n",
    "    x = 1. / (1 + np.exp(-x))\n",
    "    \n",
    "    return x\n",
    "\n",
    "def sigmoid_grad(f):\n",
    "    \"\"\"\n",
    "    Compute the gradient for the sigmoid function here. Note that\n",
    "    for this implementation, the input f should be the sigmoid\n",
    "    function value of your original input x. \n",
    "    \"\"\"\n",
    "    f = f * (1-f)\n",
    "    \n",
    "    return f\n",
    "\n",
    "def test_sigmoid_basic():\n",
    "    \"\"\"\n",
    "    Some simple tests to get you started. \n",
    "    Warning: these are not exhaustive.\n",
    "    \"\"\"\n",
    "    print \"Running basic tests...\"\n",
    "    x = np.array([[1, 2], [-1, -2]])\n",
    "    f = sigmoid(x)\n",
    "    g = sigmoid_grad(f)\n",
    "    print f\n",
    "    assert np.amax(f - np.array([[0.73105858, 0.88079708], \n",
    "        [0.26894142, 0.11920292]])) <= 1e-6\n",
    "    print g\n",
    "    assert np.amax(g - np.array([[0.19661193, 0.10499359],\n",
    "        [0.19661193, 0.10499359]])) <= 1e-6\n",
    "    print \"You should verify these results!\\n\"\n",
    "\n",
    "test_sigmoid_basic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# q2 Gradient checking\n",
    "Reference : http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sanity checks...\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def gradcheck_naive(f, x):\n",
    "    \"\"\" \n",
    "    Gradient check for a function f \n",
    "    - f should be a function that takes a single argument and outputs the cost and its gradients\n",
    "    - x is the point (numpy array) to check the gradient at\n",
    "    - Reference : http://ufldl.stanford.edu/tutorial/supervised/DebuggingGradientChecking/ \n",
    "    \"\"\" \n",
    "\n",
    "    rndstate = random.getstate()\n",
    "    random.setstate(rndstate)  \n",
    "    fx, gd = f(x) # Evaluate function value at original point\n",
    "    h = 1e-4\n",
    "\n",
    "    # Iterate over all indexes in x\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        \n",
    "        x[ix] += h\n",
    "        random.setstate(rndstate)\n",
    "        fx_p, _ = f(x)\n",
    "        x[ix] -= 2*h\n",
    "        random.setstate(rndstate)\n",
    "        fx_n, _ = f(x)\n",
    "        x[ix] += h\n",
    "        grad = (fx_p - fx_n) / (2*h)\n",
    "        \n",
    "        diff = abs(grad - gd[ix]) / max(1, abs(grad), abs(gd[ix]))\n",
    "        \n",
    "        if diff > 1e-5:\n",
    "            print \"Gradient check failed.\"\n",
    "            print \"First gradient error found at index %s\" % str(ix)\n",
    "            print \"Your gradient: %f \\t Numerical gradient: %f\" % (gd[ix], grad)\n",
    "            return\n",
    "\n",
    "    \n",
    "        it.iternext() # Step to next dimension\n",
    "\n",
    "    print \"Gradient check passed!\"\n",
    "\n",
    "def sanity_check():\n",
    "    \"\"\"\n",
    "    Some basic sanity checks.\n",
    "    \"\"\"\n",
    "    quad = lambda x: (np.sum(x ** 2), x * 2)\n",
    "\n",
    "    print \"Running sanity checks...\"\n",
    "    gradcheck_naive(quad, np.array(123.456))      # scalar test\n",
    "    gradcheck_naive(quad, np.random.randn(3,))    # 1-D test\n",
    "    gradcheck_naive(quad, np.random.randn(4,5))   # 2-D test\n",
    "    print \"\"\n",
    "\n",
    "sanity_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# q2 neural net\n",
    "Reference : http://neuralnetworksanddeeplearning.com/chap2.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sanity check...\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "def forward_backward_prop(data, labels, params, dimensions):\n",
    "    \"\"\" \n",
    "    Forward and backward propagation for a two-layer sigmoidal network \n",
    "    \n",
    "    Compute the forward propagation and for the cross entropy cost,\n",
    "    and backward propagation for the gradients for all parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    ### Unpack network parameters (do not modify)\n",
    "    ofs = 0\n",
    "    Dx, H, Dy = (dimensions[0], dimensions[1], dimensions[2])\n",
    "\n",
    "    W1 = np.reshape(params[ofs:ofs+ Dx * H], (Dx, H))\n",
    "    ofs += Dx * H\n",
    "    b1 = np.reshape(params[ofs:ofs + H], (1, H))\n",
    "    ofs += H\n",
    "    W2 = np.reshape(params[ofs:ofs + H * Dy], (H, Dy))\n",
    "    ofs += H * Dy\n",
    "    b2 = np.reshape(params[ofs:ofs + Dy], (1, Dy))\n",
    "\n",
    "    ### YOUR CODE HERE: forward propagation\n",
    "    hidden = sigmoid( np.dot(data, W1) + b1)\n",
    "    prediction = softmax( np.dot(hidden, W2) + b2)\n",
    "    cost = -np.sum(np.log(prediction) * labels)\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    ### YOUR CODE HERE: backward propagation\n",
    "    ### reference : http://neuralnetworksanddeeplearning.com/chap2.html \n",
    "    delta = prediction - labels\n",
    "    gradW2 = np.dot(hidden.T, delta) \n",
    "    gradb2 = np.sum(delta, axis = 0)\n",
    "    delta = delta.dot(W2.T) * sigmoid_grad(hidden)\n",
    "    gradW1 = data.T.dot(delta)\n",
    "    gradb1 = np.sum(delta, axis = 0)\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    ### Stack gradients (do not modify)\n",
    "    grad = np.concatenate((gradW1.flatten(), gradb1.flatten(), \n",
    "        gradW2.flatten(), gradb2.flatten()))\n",
    "    \n",
    "    return cost, grad\n",
    "\n",
    "def neural_sanity_check():\n",
    "    \"\"\"\n",
    "    Set up fake data and parameters for the neural network, and test using \n",
    "    gradcheck.\n",
    "    \"\"\"\n",
    "    print \"Running sanity check...\"\n",
    "\n",
    "    N = 20\n",
    "    dimensions = [10, 5, 10]\n",
    "    data = np.random.randn(N, dimensions[0])   # each row will be a datum\n",
    "    labels = np.zeros((N, dimensions[2]))\n",
    "    for i in xrange(N):\n",
    "        labels[i,random.randint(0,dimensions[2]-1)] = 1\n",
    "    \n",
    "    params = np.random.randn((dimensions[0] + 1) * dimensions[1] + (\n",
    "        dimensions[1] + 1) * dimensions[2], )\n",
    "\n",
    "    gradcheck_naive(lambda params: forward_backward_prop(data, labels, params,\n",
    "        dimensions), params)\n",
    "\n",
    "neural_sanity_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# q3 word2vec \n",
    "## normalize rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing normalizeRows...\n",
      "[[ 0.6         0.8       ]\n",
      " [ 0.4472136   0.89442719]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def normalizeRows(x):\n",
    "    \"\"\" Row normalization function \"\"\"\n",
    "    # Implement a function that normalizes each row of a matrix to have unit length\n",
    "    \n",
    "    length = np.sqrt( np.sum(x**2, axis = 1))\n",
    "    x /= length.reshape(x.shape[0],1)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def test_normalize_rows():\n",
    "    print \"Testing normalizeRows...\"\n",
    "    x = normalizeRows(np.array([[3.0,4.0],[1, 2]])) \n",
    "    # the result should be [[0.6, 0.8], [0.4472, 0.8944]]\n",
    "    print x\n",
    "    assert (x.all() == np.array([[0.6, 0.8], [0.4472, 0.8944]]).all())\n",
    "    print \"\"\n",
    "    \n",
    "test_normalize_rows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## softmax cost and gradient "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmaxCostAndGradient(predicted, target, outputVectors, dataset):\n",
    "    \"\"\" Softmax cost function for word2vec models \"\"\"\n",
    "    \n",
    "    # Implement the cost and gradients for one predicted word vector  \n",
    "    # and one target word vector as a building block for word2vec     \n",
    "    # models, assuming the softmax prediction function and cross      \n",
    "    # entropy loss.                                                   \n",
    "    \n",
    "    # Inputs:                                                         \n",
    "    # - predicted: numpy ndarray, predicted word vector (\\hat{v} in \n",
    "    #   the written component or \\hat{r} in an earlier version)\n",
    "    # - target: integer, the index of the target word               \n",
    "    # - outputVectors: \"output\" vectors (as rows) for all tokens     \n",
    "    # - dataset: needed for negative sampling, unused here.         \n",
    "    \n",
    "    # Outputs:                                                        \n",
    "    # - cost: cross entropy cost for the softmax word prediction    \n",
    "    # - gradPred: the gradient with respect to the predicted word   \n",
    "    #        vector                                                \n",
    "    # - grad: the gradient with respect to all the other word        \n",
    "    #        vectors                                               \n",
    "    \n",
    "    # We will not provide starter code for this function, but feel    \n",
    "    # free to reference the code you previously wrote for this        \n",
    "    # assignment!                                                  \n",
    "    \n",
    "    ### Check solution for assignment q3 for reference\n",
    "    ### predicted =  1,D ... output  =  V,D\n",
    "    ### cost is cross entropy loss of probabilities[target]\n",
    "    probabilities = softmax( np.dot(predicted, outputVectors.T))\n",
    "    cost = - np.log(probabilities[target])\n",
    "    \n",
    "    ### delta = y_predicted - y, y_i = 0 except i == target\n",
    "    delta = probabilities\n",
    "    delta[target] -= 1\n",
    "    \n",
    "    gradPred = np.dot(delta.T,outputVectors)\n",
    "    grad = np.dot(delta.reshape(delta.shape[0],1),predicted.reshape(1,predicted.shape[0]))\n",
    "    \n",
    "    return cost, gradPred, grad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# negative sample cost and gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def negSamplingCostAndGradient(predicted, target, outputVectors, dataset, \n",
    "    K=10):\n",
    "    \"\"\" Negative sampling cost function for word2vec models \"\"\"\n",
    "\n",
    "    # Implement the cost and gradients for one predicted word vector  \n",
    "    # and one target word vector as a building block for word2vec     \n",
    "    # models, using the negative sampling technique. K is the sample  \n",
    "    # size. You might want to use dataset.sampleTokenIdx() to sample  \n",
    "    # a random word index. \n",
    "    # \n",
    "    # Note: See test_word2vec below for dataset's initialization.\n",
    "    #                                       \n",
    "    # Input/Output Specifications: same as softmaxCostAndGradient     \n",
    "    # We will not provide starter code for this function, but feel    \n",
    "    # free to reference the code you previously wrote for this        \n",
    "    # assignment!\n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    out_indices = [target]\n",
    "\n",
    "    for k in range(K):\n",
    "        index = dataset.sampleTokenIdx()\n",
    "        while index == target:\n",
    "            index = dataset.sampleTokenIdx()\n",
    "        out_indices += [index]\n",
    "\n",
    "    out_vectors = outputVectors[out_indices,:]\n",
    "\n",
    "    labels = np.array([1] + [-1]* K)\n",
    "\n",
    "    prediction = sigmoid( out_vectors.dot(predicted) * labels )\n",
    "    cost = - np.sum(np.log(prediction))\n",
    "\n",
    "    delta = labels * (prediction - 1)\n",
    "\n",
    "\n",
    "    gradPred = delta.dot( out_vectors)\n",
    "\n",
    "    gradTemp = (delta.reshape(K+1, 1)).dot(predicted.reshape(1, predicted.shape[0]))\n",
    "    ### END YOUR CODE\n",
    "\n",
    "    grad = np.zeros(outputVectors.shape)\n",
    "    for index in range(K+1):\n",
    "        grad[out_indices[index]] += gradTemp[index]\n",
    "\n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradPred, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skipgram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def skipgram(currentWord, C, contextWords, tokens, inputVectors, outputVectors, \n",
    "    dataset, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "    \"\"\" Skip-gram model in word2vec \"\"\"\n",
    "\n",
    "    # Implement the skip-gram model in this function.\n",
    "\n",
    "    # Inputs:                                                         \n",
    "    # - currrentWord: a string of the current center word           \n",
    "    # - C: integer, context size                                    \n",
    "    # - contextWords: list of no more than 2*C strings, the context words                                               \n",
    "    # - tokens: a dictionary that maps words to their indices in    \n",
    "    #      the word vector list                                \n",
    "    # - inputVectors: \"input\" word vectors (as rows) for all tokens           \n",
    "    # - outputVectors: \"output\" word vectors (as rows) for all tokens         \n",
    "    # - word2vecCostAndGradient: the cost and gradient function for \n",
    "    #      a prediction vector given the target word vectors,  \n",
    "    #      could be one of the two cost functions you          \n",
    "    #      implemented above\n",
    "\n",
    "    # Outputs:                                                        \n",
    "    # - cost: the cost function value for the skip-gram model       \n",
    "    # - grad: the gradient with respect to the word vectors         \n",
    "    # We will not provide starter code for this function, but feel    \n",
    "    # free to reference the code you previously wrote for this        \n",
    "    # assignment!\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    # v_c\n",
    "    index_center = tokens[currentWord]\n",
    "    predicted = inputVectors[index_center, :]  \n",
    "    \n",
    "    cost = 0\n",
    "    gradIn = np.zeros(inputVectors.shape)\n",
    "    gradOut = np.zeros(outputVectors.shape)\n",
    "    \n",
    "    \n",
    "    for word in contextWords:\n",
    "        index = tokens[word]\n",
    "        cost_w, gradP, grad = word2vecCostAndGradient(predicted, index, outputVectors, dataset)\n",
    "        cost += cost_w\n",
    "        gradIn[index_center, : ] += gradP\n",
    "        gradOut += grad\n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradIn, gradOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CBOW model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cbow(currentWord, C, contextWords, tokens, inputVectors, outputVectors, \n",
    "    dataset, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "    \"\"\" CBOW model in word2vec \"\"\"\n",
    "\n",
    "    # Implement the continuous bag-of-words model in this function.            \n",
    "    # Input/Output specifications: same as the skip-gram model        \n",
    "    # We will not provide starter code for this function, but feel    \n",
    "    # free to reference the code you previously wrote for this        \n",
    "    # assignment!\n",
    "\n",
    "    #################################################################\n",
    "    # IMPLEMENTING CBOW IS EXTRA CREDIT, DERIVATIONS IN THE WRIITEN #\n",
    "    # ASSIGNMENT ARE NOT!                                           #  \n",
    "    #################################################################\n",
    "    \n",
    "    cost = 0\n",
    "    gradIn = np.zeros(inputVectors.shape)\n",
    "    gradOut = np.zeros(outputVectors.shape)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    indexes = [tokens[word] for word in contextWords]\n",
    "    predicted = np.zeros(( inputVectors.shape[1]))\n",
    "    \n",
    "    for index in indexes:\n",
    "        predicted += inputVectors[index,:]\n",
    "    \n",
    "    #print \"indexes: \" ,indexes\n",
    "    \n",
    "    cost, gradP, gradOut = word2vecCostAndGradient(predicted, tokens[currentWord], outputVectors, dataset)\n",
    "    \n",
    "    for index in indexes:\n",
    "        gradIn[index,:] += gradP \n",
    "    \n",
    "    ### END YOUR CODE\n",
    "    \n",
    "    return cost, gradIn, gradOut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Gradient check for skip-gram ====\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "\n",
      "==== Gradient check for CBOW      ====\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "\n",
      "=== Results ===\n",
      "(11.166109001533981, array([[ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [-1.26947339, -1.36873189,  2.45158957],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[-0.41045956,  0.18834851,  1.43272264],\n",
      "       [ 0.38202831, -0.17530219, -1.33348241],\n",
      "       [ 0.07009355, -0.03216399, -0.24466386],\n",
      "       [ 0.09472154, -0.04346509, -0.33062865],\n",
      "       [-0.13638384,  0.06258276,  0.47605228]]))\n",
      "(13.959405258751875, array([[ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [-4.12113804, -1.67347865, -1.5049951 ],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[-0.49853822,  0.22876535,  1.74016407],\n",
      "       [-0.02700439,  0.01239157,  0.09425972],\n",
      "       [-0.68292656,  0.31337605,  2.38377767],\n",
      "       [-0.84273629,  0.3867083 ,  2.94159878],\n",
      "       [-0.16124059,  0.07398883,  0.5628156 ]]))\n",
      "(0.79899580109066481, array([[ 0.23330542, -0.51643128, -0.8281311 ],\n",
      "       [ 0.11665271, -0.25821564, -0.41406555],\n",
      "       [ 0.11665271, -0.25821564, -0.41406555],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[ 0.80954933,  0.21962514, -0.54095764],\n",
      "       [-0.03556575, -0.00964874,  0.02376577],\n",
      "       [-0.13016109, -0.0353118 ,  0.08697634],\n",
      "       [-0.1650812 , -0.04478539,  0.11031068],\n",
      "       [-0.47874129, -0.1298792 ,  0.31990485]]))\n",
      "(7.7630889928618743, array([[-3.24112111, -1.89068433, -2.69507064],\n",
      "       [-1.62056055, -0.94534217, -1.34753532],\n",
      "       [-1.62056055, -0.94534217, -1.34753532],\n",
      "       [ 0.        ,  0.        ,  0.        ],\n",
      "       [ 0.        ,  0.        ,  0.        ]]), array([[ 0.21992784,  0.0596649 , -0.14696034],\n",
      "       [-1.37825047, -0.37390982,  0.92097553],\n",
      "       [-1.55404334, -0.42160121,  1.03844397],\n",
      "       [-1.72636934, -0.46835207,  1.15359577],\n",
      "       [-2.36749007, -0.64228369,  1.58200593]]))\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "# Testing functions below. DO NOT MODIFY!   #\n",
    "#############################################\n",
    "\n",
    "def word2vec_sgd_wrapper(word2vecModel, tokens, wordVectors, dataset, C, word2vecCostAndGradient = softmaxCostAndGradient):\n",
    "    batchsize = 50\n",
    "    cost = 0.0\n",
    "    grad = np.zeros(wordVectors.shape)\n",
    "    N = wordVectors.shape[0]\n",
    "    inputVectors = wordVectors[:N/2,:]\n",
    "    outputVectors = wordVectors[N/2:,:]\n",
    "    for i in xrange(batchsize):\n",
    "        C1 = random.randint(1,C)\n",
    "        centerword, context = dataset.getRandomContext(C1)\n",
    "        \n",
    "        if word2vecModel == skipgram:\n",
    "            denom = 1\n",
    "        else:\n",
    "            denom = 1\n",
    "        \n",
    "        c, gin, gout = word2vecModel(centerword, C1, context, tokens, inputVectors, outputVectors, dataset, word2vecCostAndGradient)\n",
    "        cost += c / batchsize / denom\n",
    "        grad[:N/2, :] += gin / batchsize / denom\n",
    "        grad[N/2:, :] += gout / batchsize / denom\n",
    "        \n",
    "    return cost, grad\n",
    "\n",
    "def test_word2vec():\n",
    "    # Interface to the dataset for negative sampling\n",
    "    dataset = type('dummy', (), {})()\n",
    "    def dummySampleTokenIdx():\n",
    "        return random.randint(0, 4)\n",
    "\n",
    "    def getRandomContext(C):\n",
    "        tokens = [\"a\", \"b\", \"c\", \"d\", \"e\"]\n",
    "        return tokens[random.randint(0,4)], [tokens[random.randint(0,4)] \\\n",
    "           for i in xrange(2*C)]\n",
    "    dataset.sampleTokenIdx = dummySampleTokenIdx\n",
    "    dataset.getRandomContext = getRandomContext\n",
    "\n",
    "    random.seed(31415)\n",
    "    np.random.seed(9265)\n",
    "    dummy_vectors = normalizeRows(np.random.randn(10,3))\n",
    "    dummy_tokens = dict([(\"a\",0), (\"b\",1), (\"c\",2),(\"d\",3),(\"e\",4)])\n",
    "    print \"==== Gradient check for skip-gram ====\"\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(skipgram, dummy_tokens, vec, dataset, 5), dummy_vectors)\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(skipgram, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient), dummy_vectors)\n",
    "    print \"\\n==== Gradient check for CBOW      ====\"\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(cbow, dummy_tokens, vec, dataset, 5), dummy_vectors)\n",
    "    gradcheck_naive(lambda vec: word2vec_sgd_wrapper(cbow, dummy_tokens, vec, dataset, 5, negSamplingCostAndGradient), dummy_vectors)\n",
    "\n",
    "    print \"\\n=== Results ===\"\n",
    "    print skipgram(\"c\", 3, [\"a\", \"b\", \"e\", \"d\", \"b\", \"c\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset)\n",
    "    print skipgram(\"c\", 1, [\"a\", \"b\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset, negSamplingCostAndGradient)\n",
    "    print cbow(\"a\", 2, [\"a\", \"b\", \"c\", \"a\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset)\n",
    "    print cbow(\"a\", 2, [\"a\", \"b\", \"a\", \"c\"], dummy_tokens, dummy_vectors[:5,:], dummy_vectors[5:,:], dataset, negSamplingCostAndGradient)\n",
    "\n",
    "test_word2vec()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sanity checks...\n",
      "iter 100: 0.004578\n",
      "iter 200: 0.004353\n",
      "iter 300: 0.004136\n",
      "iter 400: 0.003929\n",
      "iter 500: 0.003733\n",
      "iter 600: 0.003546\n",
      "iter 700: 0.003369\n",
      "iter 800: 0.003200\n",
      "iter 900: 0.003040\n",
      "iter 1000: 0.002888\n",
      "test 1 result: 8.41483678608e-10\n",
      "iter 100: 0.000000\n",
      "iter 200: 0.000000\n",
      "iter 300: 0.000000\n",
      "iter 400: 0.000000\n",
      "iter 500: 0.000000\n",
      "iter 600: 0.000000\n",
      "iter 700: 0.000000\n",
      "iter 800: 0.000000\n",
      "iter 900: 0.000000\n",
      "iter 1000: 0.000000\n",
      "test 2 result: 0.0\n",
      "iter 100: 0.041205\n",
      "iter 200: 0.039181\n",
      "iter 300: 0.037222\n",
      "iter 400: 0.035361\n",
      "iter 500: 0.033593\n",
      "iter 600: 0.031913\n",
      "iter 700: 0.030318\n",
      "iter 800: 0.028802\n",
      "iter 900: 0.027362\n",
      "iter 1000: 0.025994\n",
      "test 3 result: -2.52445103582e-09\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Save parameters every a few SGD iterations as fail-safe\n",
    "SAVE_PARAMS_EVERY = 1000\n",
    "\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import os.path as op\n",
    "import cPickle as pickle\n",
    "\n",
    "def load_saved_params():\n",
    "    \"\"\" A helper function that loads previously saved parameters and resets iteration start \"\"\"\n",
    "    st = 0\n",
    "    for f in glob.glob(\"saved_params_*.npy\"):\n",
    "        iter = int(op.splitext(op.basename(f))[0].split(\"_\")[2])\n",
    "        if (iter > st):\n",
    "            st = iter\n",
    "            \n",
    "    if st > 0:\n",
    "        with open(\"saved_params_%d.npy\" % st, \"r\") as f:\n",
    "            params = pickle.load(f)\n",
    "            state = pickle.load(f)\n",
    "        return st, params, state\n",
    "    else:\n",
    "        return st, None, None\n",
    "    \n",
    "def save_params(iter, params):\n",
    "    with open(\"saved_params_%d.npy\" % iter, \"w\") as f:\n",
    "        pickle.dump(params, f)\n",
    "        pickle.dump(random.getstate(), f)\n",
    "\n",
    "def sgd(f, x0, step, iterations, postprocessing = None, useSaved = False, PRINT_EVERY=10):\n",
    "    \"\"\" Stochastic Gradient Descent \"\"\"\n",
    "    # Implement the stochastic gradient descent method in this        \n",
    "    # function.                                                       \n",
    "    \n",
    "    # Inputs:                                                         \n",
    "    # - f: the function to optimize, it should take a single        \n",
    "    #     argument and yield two outputs, a cost and the gradient  \n",
    "    #     with respect to the arguments                            \n",
    "    # - x0: the initial point to start SGD from                     \n",
    "    # - step: the step size for SGD                                 \n",
    "    # - iterations: total iterations to run SGD for                 \n",
    "    # - postprocessing: postprocessing function for the parameters  \n",
    "    #     if necessary. In the case of word2vec we will need to    \n",
    "    #     normalize the word vectors to have unit length.          \n",
    "    # - PRINT_EVERY: specifies every how many iterations to output  \n",
    "\n",
    "    # Output:                                                         \n",
    "    # - x: the parameter value after SGD finishes  \n",
    "    \n",
    "    # Anneal learning rate every several iterations\n",
    "    ANNEAL_EVERY = 20000\n",
    "    \n",
    "    if useSaved:\n",
    "        start_iter, oldx, state = load_saved_params()\n",
    "        if start_iter > 0:\n",
    "            x0 = oldx;\n",
    "            step *= 0.5 ** (start_iter / ANNEAL_EVERY)\n",
    "            \n",
    "        if state:\n",
    "            random.setstate(state)\n",
    "    else:\n",
    "        start_iter = 0\n",
    "    \n",
    "    x = x0\n",
    "    \n",
    "    if not postprocessing:\n",
    "        postprocessing = lambda x: x\n",
    "    \n",
    "    expcost = None\n",
    "    \n",
    "    for iter in xrange(start_iter + 1, iterations + 1):\n",
    "        ### Don't forget to apply the postprocessing after every iteration!\n",
    "        ### You might want to print the progress every few iterations.\n",
    "\n",
    "        cost = None\n",
    "        ### YOUR CODE HERE\n",
    "\n",
    "        cost, grad = f(x)\n",
    "        x -= step * grad\n",
    "        \n",
    "        x = postprocessing(x)\n",
    "        \n",
    "        ### END YOUR CODE\n",
    "        \n",
    "        if iter % PRINT_EVERY == 0:\n",
    "            if not expcost:\n",
    "                expcost = cost\n",
    "            else:\n",
    "                expcost = .95 * expcost + .05 * cost\n",
    "            print \"iter %d: %f\" % (iter, expcost)\n",
    "        \n",
    "        if iter % SAVE_PARAMS_EVERY == 0 and useSaved:\n",
    "            save_params(iter, x)\n",
    "            \n",
    "        if iter % ANNEAL_EVERY == 0:\n",
    "            step *= 0.5\n",
    "    \n",
    "    return x\n",
    "\n",
    "def sanity_check():\n",
    "    quad = lambda x: (np.sum(x ** 2), x * 2)\n",
    "\n",
    "    print \"Running sanity checks...\"\n",
    "    t1 = sgd(quad, 0.5, 0.01, 1000, PRINT_EVERY=100)\n",
    "    print \"test 1 result:\", t1\n",
    "    assert abs(t1) <= 1e-6\n",
    "\n",
    "    t2 = sgd(quad, 0.0, 0.01, 1000, PRINT_EVERY=100)\n",
    "    print \"test 2 result:\", t2\n",
    "    assert abs(t2) <= 1e-6\n",
    "\n",
    "    t3 = sgd(quad, -1.5, 0.01, 1000, PRINT_EVERY=100)\n",
    "    print \"test 3 result:\", t3\n",
    "    assert abs(t3) <= 1e-6\n",
    "    \n",
    "    print \"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sanity_check();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Show time! Now we are going to load some real data and train word vectors with everything you just implemented!**\n",
    "\n",
    "We are going to use the Stanford Sentiment Treebank (SST) dataset to train word vectors, and later apply them to a simple sentiment analysis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cs224d.data_utils import *\n",
    "# Reset the random seed to make sure that everyone gets the same results\n",
    "random.seed(314)\n",
    "dataset = StanfordSentiment()\n",
    "tokens = dataset.tokens()\n",
    "nWords = len(tokens)\n",
    "\n",
    "# We are going to train 10-dimensional vectors for this assignment\n",
    "dimVectors = 10\n",
    "\n",
    "# Context size\n",
    "C = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sanity check: cost at convergence should be around or below 10\n"
     ]
    }
   ],
   "source": [
    "#Train word vectors\n",
    "\n",
    "# Reset the random seed to make sure that everyone gets the same results\n",
    "random.seed(31415)\n",
    "np.random.seed(9265)\n",
    "wordVectors = np.concatenate(((np.random.rand(nWords, dimVectors) - .5) / \\\n",
    "\tdimVectors, np.zeros((nWords, dimVectors))), axis=0)\n",
    "wordVectors0 = sgd(\n",
    "    lambda vec: word2vec_sgd_wrapper(skipgram, tokens, vec, dataset, C, \n",
    "    \tnegSamplingCostAndGradient), \n",
    "    wordVectors, 0.3, 20000, None, True, PRINT_EVERY=1000)\n",
    "print \"sanity check: cost at convergence should be around or below 10\"\n",
    "\n",
    "# sum the input and output word vectors\n",
    "wordVectors = (wordVectors0[:nWords,:] + wordVectors0[nWords:,:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.13902013058402479, 0.18476601867317541)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEDCAYAAADdpATdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10675e910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Visualize the word vectors you trained\n",
    "_, wordVectors0, _ = load_saved_params()\n",
    "wordVectors = (wordVectors0[:nWords,:] + wordVectors0[nWords:,:])\n",
    "visualizeWords = [\"the\", \"a\", \"an\", \",\", \".\", \"?\", \"!\", \"``\", \"''\", \"--\", \n",
    "\t\"good\", \"great\", \"cool\", \"brilliant\", \"wonderful\", \"well\", \"amazing\",\n",
    "\t\"worth\", \"sweet\", \"enjoyable\", \"boring\", \"bad\", \"waste\", \"dumb\", \n",
    "\t\"annoying\"]\n",
    "visualizeIdx = [tokens[word] for word in visualizeWords]\n",
    "visualizeVecs = wordVectors[visualizeIdx, :]\n",
    "temp = (visualizeVecs - np.mean(visualizeVecs, axis=0))\n",
    "covariance = 1.0 / len(visualizeIdx) * temp.T.dot(temp)\n",
    "U,S,V = np.linalg.svd(covariance)\n",
    "coord = temp.dot(U[:,0:2]) \n",
    "\n",
    "for i in xrange(len(visualizeWords)):\n",
    "    plt.text(coord[i,0], coord[i,1], visualizeWords[i], \n",
    "    \tbbox=dict(facecolor='green', alpha=0.1))\n",
    "    \n",
    "plt.xlim((np.min(coord[:,0]), np.max(coord[:,0])))\n",
    "plt.ylim((np.min(coord[:,1]), np.max(coord[:,1])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
